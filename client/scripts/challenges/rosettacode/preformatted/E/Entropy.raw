
/* eslint spaced-comment: 0 */
/* eslint no-redeclare: 0 */
/* eslint no-unused-vars: 0 */

const assert = require('assert');

/// title: Entropy
/// type: rosetta-code

/// categories:


/// difficulty: ?

/// benchmark:
replaceWithActualFunctionHere;

/// description:
/// <div class="rosetta"><br/><dl class="rosetta__description-list"><dt class="rosetta__description-title">Task:</dt></dl>
/// <p class="rosetta__paragraph">Calculate the Shannon entropy  H  of a given input string.</p><br/><p class="rosetta__paragraph">Given the discreet random variable $X$ that is a string of $N$ "symbols" (total characters) consisting of $n$ different characters (n=2 for binary), the Shannon entropy of X in <span class="rosetta__text--bold">bits/symbol</span> is :</p>
/// <p class="rosetta__paragraph"><span class="rosetta__text--indented">$H_2(X) = -\sum_{i=1}^n \frac{count_i}{N} \log_2 \left(\frac{count_i}{N}\right)$</span></p><br/><p class="rosetta__paragraph">where $count_i$ is the count of character $n_i$.</p><br/><p class="rosetta__paragraph">For this task, use X="<tt>1223334444</tt>" as an example. The result should be 1.84644... bits/symbol.  This assumes X was a random variable, which may not be the case, or it may depend on the observer.</p><br/><p class="rosetta__paragraph">This coding problem calculates the "specific" or "<a class="rosetta__link--wiki" href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties" title="wp: Intensive_and_extensive_properties">intensive</a>" entropy that finds its parallel in physics with "specific entropy" S<sup>0</sup> which is entropy per kg or per mole, not like physical entropy S and therefore not the "information" content of a file. It comes from Boltzmann's H-theorem where $S=k_B  N  H$ where N=number of molecules. Boltzmann's H is the same equation as Shannon's H, and it gives the specific entropy H on a "per molecule" basis.</p><br/><p class="rosetta__paragraph">The "total", "absolute", or "<a class="rosetta__link--wiki" href="https://en.wikipedia.org/wiki/Intensive_and_extensive_properties" title="wp: Intensive_and_extensive_properties">extensive</a>" information entropy is</p>
/// <p class="rosetta__paragraph"><span class="rosetta__text--indented">$S=H_2 N$ bits</span></p>
/// <p class="rosetta__paragraph">This is not the entropy being coded here, but it is the closest to physical entropy and a measure of the information content of a string. But it does not look for any patterns that might be available for compression, so it is a very restricted, basic, and certain measure of "information". Every binary file with an equal number of 1's and 0's will have S=N bits. All hex files with equal symbol frequencies will have $S=N \log_2(16)$ bits of entropy. The total entropy in bits of the example above is S= 10*18.4644 = 18.4644  bits.</p><br/><p class="rosetta__paragraph">The H function does not look for any patterns in data or check if X was a random variable.  For example, X=000000111111 gives the same calculated entropy in all senses as Y=010011100101. For most purposes it is usually more relevant to divide the gzip length by the length of the original data to get an informal measure of how much "order" was in the data.</p><br/><p class="rosetta__paragraph">Two other "entropies" are useful:</p><br/><p class="rosetta__paragraph">Normalized specific entropy:</p>
/// <p class="rosetta__paragraph"><span class="rosetta__text--indented">$H_n=\frac{H_2 * \log(2)}{\log(n)}$ </span></p>
/// <p class="rosetta__paragraph">which varies from 0 to 1 and it has units of "entropy/symbol" or just 1/symbol. For this example, H<sub>n<\sub>= 0.923.</p><br/><p class="rosetta__paragraph">Normalized total (extensive) entropy:</p>
/// <p class="rosetta__paragraph"><span class="rosetta__text--indented">$S_n = \frac{H_2 N * \log(2)}{\log(n)}$</span></p>
/// <p class="rosetta__paragraph">which varies from 0 to N and does not have units. It is simply the "entropy", but it needs to be called "total normalized extensive entropy" so that it is not confused with Shannon's (specific) entropy or physical entropy. For this example, S<sub>n<\sub>= 9.23.</p><br/><p class="rosetta__paragraph">Shannon himself is the reason his "entropy/symbol" H function is very confusingly called "entropy". That's like calling a function that returns a speed a "meter". See section 1.7 of his classic <a class="rosetta__link--wiki" href="http://worrydream.com/refs/Shannon%20-%20A%20Mathematical%20Theory%20of%20Communication.pdf" title="link: http://worrydream.com/refs/Shannon%20-%20A%20Mathematical%20Theory%20of%20Communication.pdf">A Mathematical Theory of Communication</a> and search on "per symbol" and "units" to see he always stated his entropy H has units of "bits/symbol" or "entropy/symbol" or "information/symbol".  So it is legitimate to say entropy NH is "information".</p><br/><p class="rosetta__paragraph">In keeping with Landauer's limit, the physics entropy generated from erasing N bits is $S = H_2 N k_B \ln(2)$ if the bit storage device is perfectly efficient.  This can be solved for H<sub>2</sub>*N to (arguably) get the number of bits of information that a physical entropy represents.</p><br/><dl class="rosetta__description-list"><dt class="rosetta__description-title">Related tasks: </dt></dl>
/// <p class="rosetta__paragraph"><span class="rosetta__text--indented">* <a class="rosetta__link--rosetta" href="http://rosettacode.org/wiki/Fibonacci_word" title="Fibonacci_word">Fibonacci_word</a></span></p>
/// <p class="rosetta__paragraph"><span class="rosetta__text--indented">* <a class="rosetta__link--rosetta" href="http://rosettacode.org/wiki/Entropy/Narcissist" title="Entropy/Narcissist">Entropy/Narcissist</a></span></p>
/// <br><br><br/></div>

/// challengeSeed:
function replaceMe (foo) {
  // Good luck!
  return true;
}

/// solutions:
(function(shannon) {
  // Create a dictionary of character frequencies and iterate over it.
  function process(s, evaluator) {
    var h = Object.create(null), k;
    s.split('').forEach(function(c) {
      h[c] && h[c]++ || (h[c] = 1); });
    if (evaluator) for (k in h) evaluator(k, h[k]);
    return h;
  };
  // Measure the entropy of a string in bits per symbol.
  shannon.entropy = function(s) {
    var sum = 0,len = s.length;
    process(s, function(k, f) {
      var p = f/len;
      sum -= p * Math.log(p) / Math.log(2);
    });
    return sum;
  };
})(window.shannon = window.shannon || {});
 
// Log the Shannon entropy of a string.
function logEntropy(s) {
  console.log('Entropy of "' + s + '" in bits per symbol:', shannon.entropy(s));
}
 
logEntropy('1223334444');
logEntropy('0');
logEntropy('01');
logEntropy('0123');
logEntropy('01234567');
logEntropy('0123456789abcdef');

/// rawSolutions:
=={{header|JavaScript}}==
{{works with|ECMA-262 (5.1)}}
The proces function builds a histogram of character frequencies then iterates over it.

The entropy function calls into process and evaluates the frequencies as they're passed back.
<lang JavaScript>(function(shannon) {
  // Create a dictionary of character frequencies and iterate over it.
  function process(s, evaluator) {
    var h = Object.create(null), k;
    s.split('').forEach(function(c) {
      h[c] && h[c]++ || (h[c] = 1); });
    if (evaluator) for (k in h) evaluator(k, h[k]);
    return h;
  };
  // Measure the entropy of a string in bits per symbol.
  shannon.entropy = function(s) {
    var sum = 0,len = s.length;
    process(s, function(k, f) {
      var p = f/len;
      sum -= p * Math.log(p) / Math.log(2);
    });
    return sum;
  };
})(window.shannon = window.shannon || {});
 
// Log the Shannon entropy of a string.
function logEntropy(s) {
  console.log('Entropy of "' + s + '" in bits per symbol:', shannon.entropy(s));
}
 
logEntropy('1223334444');
logEntropy('0');
logEntropy('01');
logEntropy('0123');
logEntropy('01234567');
logEntropy('0123456789abcdef');</lang>
{{out}}
<pre>Entropy of "1223334444" in bits per symbol: 1.8464393446710154
Entropy of "0" in bits per symbol: 0
Entropy of "01" in bits per symbol: 1
Entropy of "0123" in bits per symbol: 2
Entropy of "01234567" in bits per symbol: 3
Entropy of "0123456789abcdef" in bits per symbol: 4</pre>



/// tail:
const replaceThis = 3;

/// tests:
assert(typeof replaceMe === 'function', 'message: <code>replaceMe</code> is a function.');
